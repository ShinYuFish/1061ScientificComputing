\documentclass[12pt,a4paper,draft]{article}

\usepackage{geometry}
\usepackage{fancyhdr}   %Required for custom header and footer 
\usepackage{titlesec}     %Required for changing style of section/subsection
\usepackage{ulem}
\usepackage{amsmath,amsfonts,amssymb} %Requered for some specific math mode
\usepackage{enumitem}

%setup layout of the document
\geometry{a4paper,margin=0.7 in ,top = 1 in}
\linespread{1.2}  %行距

%set up the header and footer
\pagestyle{fancy}
\lhead{\classname \ Note}
\chead{}
\rhead{\noteauthor}
\cfoot{\thepage}
\renewcommand{\headrulewidth}{0.6pt} %size of header rule
\renewcommand{\footrulewidth}{0.6pt} %size of footer rule

% changing style of section caption
\renewcommand\thesection{\arabic{section}.} 
\renewcommand\thesubsection{\alph{subsection}}
\titleformat{\section}
	{\Large\bfseries}{\thesection}{.5em}{}
%\titleformat{\subsection}[runin]
%	{\normalfont\fontfamily{}{\fontsize{12}{17}}}{{(}\thesubsection{)}}{.5em}{}



%name and title
\newcommand{\classname}{Scientific Computing}
\newcommand{\noteauthor}{Shin Yu Chen}


\begin{document}
\begin{center}
\LARGE\textbf{\classname \ Semester Review}
\end{center}
\section{MATLAB Basic Command}%done
\begin{itemize}
\item 2D plot, 3D plot
\item m-file function
\item array,matrix manipulation(vector calculation in MATLAB)
\end{itemize}
\section{Financial Computing}%done
\begin{itemize}
\item Compounding: basic formula:$f=p(1+r)^n$/rule of 70/rule of 200/how to compute mortgage 
\item Internal Rate of Return: The rate at an  investment plan breaks even$$NPV = \sum_{i=0}^n \cfrac{c_i}{(1+r)^i} = 0\qquad NFV = \sum_{i=0}^n c_i(1+r)^{n-i} = 0$$
\end{itemize}
\section{K-means clustering}%done
\begin{itemize}
\item Objective: Deduction for unlabeled data, difference between input data set and cluster center should be as small as possible.
\item Objective function:$$J(X_{d\times n};C_{d\times m,m<<n},A_{n\times m}) = \sum_{j=1}^m \sum_{i=1}^n a_{ij}\parallel x_i -c_j \parallel^2 \qquad A:assignment\  matrix$$
\item Step: fixed C to find best A, fixed A to find best C, repeat above til converge
$$a_{ij}\begin{cases} 1\ if j=arg \ min \parallel x_i -c_q \parallel^2\\ 0,otherwise\end{cases} \qquad c_i = \cfrac{\sum\limits_{i=1}^n a_{ij}x_i}{\sum\limits_{i=1}^na_{ij}}$$
* guarantee to converge!(J is reduced repeatedly)
\end{itemize}
\section{Image Compression}%done
\begin{itemize}
\item True color: Each pixel is represented by [R,G,B]vector\\ Index-color: Each pixel is represented by an index into a color map of $2^b$ colors
\item Compression Ratio
$$ \rho = \cfrac{before}{after} = \cfrac{m*n*3*8}{m*n*\log_2c+c*3*8}\approx\cfrac{24}{\log_2c}$$
\item Blocked Base Compression Ratio(application of k-means clustering):
 $$ \rho = \cfrac{before}{after} = \cfrac{m*n*3*8}{\cfrac{m}{q}*\cfrac{n}{q}*\log_2c+c*3*8}\approx\cfrac{24q^2}{\log_2c+\cfrac{24cq^4}{m*n}}$$
\end{itemize}
\section{MLE for PDF}%done
\begin{itemize}
\item Maximum Likelihood Estimation: A way to find statistical model with optimum data\\
$\Rightarrow$ Formulate objective function and derive parameter\\ 
$\Rightarrow$ Inequality of Arithmetic and Geometric Means
$$\cfrac{\sum\limits_{i=1}^{n}x_i}{n} \geq \Big(\prod_{i=1}^{n} x_i \Big)^{1/n}$$ equality holds only when $x_1 = x_2 = \cdots = x_n$
\item Probability Density Function:
\begin{itemize}
\item $J(p,q,r) = p^{n1}q^{n2}r^{n3}$ ,with $p+q+r =1$,$p,q,r\geq 0$
\item Common- Guassian Distribution:$g(x,\mu,\sigma^2) = \cfrac{1}{\sigma\sqrt{2\pi}}\ exp[-0.5(\cfrac{x-\mu}{\sigma})^2]$ \\MLE: $\mu = \cfrac{1}{n}\sum x_i \quad \sigma^2 = \cfrac{1}{n}\sum (x_i - \mu)^2$
\end{itemize}
\end{itemize}
\section{ML for Classification}
\begin{itemize}
\item K-nearest Neighbour Classifier: Find first k nearest neighbour of a given point, determine the class by voting among those points.
\begin{itemize}
\item Voronoi Diagram
\item PROS:intuitive/no computation for model construction \\ CONS:massive computation when data is big/no straightforward way 
\end{itemize} 
\item Quadratic Classifiers: Select a class of parameterized PDF then identify parameters via MLE\\
$\Rightarrow$ test stage: $C = arg\ maxPr(C)*pdf_c(x)$
\begin{itemize}
\item Characteristic: decision boundary is quadratic function
\item PROS: easy compute when data is small/easy for LOOCV\\
CONS: covariance matrix is hard to compute or not exist/cannot handle bi-model data
\end{itemize}
\item Naive Bayes Classifiers:\\Identify class pdf $pdf_c = pdf_{1,c}*pdf_{2,c}*\cdots*pdf_{d,c}$\\
$\Rightarrow$ test stage: $C = arg\ maxPr(C)*pdf_c(x)$
\begin{itemize}
\item Characteristic: statistical independent/each feature governed by a PDF
\item PROS:Fast computation/Robust than QC\\
CONS:not able to deal bi-model/class boundary not complex as QC
\end{itemize}

\item Deep Neural Network: given desired i/o pairs and construct a model with it(structure identification \&{} parameter identification)\\
$\Rightarrow$  Multilayer perceptron
\begin{itemize}
\item >3 layers(deep neural network) for arbitrary regions
\item Training: gradient descent(check:sec \ref{sec:GD})/Gauss-Newton method
\item Chain rule \& partial derivative to implement a basic math model
\end{itemize}
\item Performance Evaluation: recognition rate$\uparrow$,error rate$\downarrow$,computation load$\downarrow$
\begin{itemize}
\item Stage:training set/validation set/test set
\item Method:inside test/one-side holdout test/two-side holdout test/M-fold cross validation/leave one-out cross validation 
\end{itemize}
\end{itemize}
\section{Data Fitting and Regression Analysis} %done
\begin{itemize}
\item Data fitting :
\begin{enumerate}
\item Using \texttt{\textbackslash} to solve basic regression problem $A\theta = b\ \Rightarrow \ \theta = A\textbackslash b$
\item Error sum function is the  objective function we want to minimize, can be shown as sum of squared error$\sum [y_i -f(x_i)]^2$
\item MATLAB provide \texttt{polyfit} and \texttt{polyval} to simplify our codes.
\end{enumerate}
\item Least-squares Estimate 
\begin{enumerate}
\item $E(\theta) = (A\theta-b)^T(A\theta-b)$ $\nabla_\theta(\theta) = 0 $ $\Rightarrow \theta = (A^TA)^{-1}A^Tb$
\item Application: distance to a line/plane \\
$d^2 = \Big(\cfrac{a}{\sqrt{a^2+b^2}}\ x_0+\cfrac{b}{\sqrt{a^2+b^2}}\ y_0-\cfrac{c}{\sqrt{a^2+b^2}}\Big)^2$
\end{enumerate}
\item Non-linear Regression Problem
$\Rightarrow $ Transformation\. example: $y=ae^{bx} \Rightarrow ln y=ln a +bx$ 
\end{itemize}
\section{Principal Component Analysis} %done
\begin{itemize}
\item Objective: Reduction of unlabelled data with dimensionality reduction, data variance $\uparrow$ \\
* not design for clasification problem
\item i/o: zero justified dataset $\Rightarrow$ unit vector \textbf{u }that square sum of dataset's projection onto \textbf{u} is maximized
\item Application: line/plane fitting, machine learning, face recognition
\item PCA for Total Least Squares: minimum projection on \textbf{y} means maximum projection on \textbf{x}, so \textbf{y} is the normal vector 
\end{itemize}
\section{Page Rank} %done
\begin{itemize}
\item Connectivity Matrix(row:in/column:out)$\Rightarrow$ Transition Probability Matrix \\
$A = pA_1+(1-p)A_2 \qquad A_1$:follow with the link $A_2$:jump to random page\\
\textit{PageRank}: z for initial state, $Az,A^2z...A^kz$,until $A^{k+1}z=A^kz$
\item Compute Pagerank: eigenvector method/powermethod(for large n) 
\item Eigenvalue and Characteristic
\begin{enumerate}
\item Row,Column sum with eigenvalue and eigenvector
\item Eigenvalue Decomposition $A^k = VD^kV^{-1}$
\end{enumerate}
\item Application: Team ranking, university rank,recommendation
\end{itemize}
\section{Audio Basic} %done
\begin{itemize}
\item Parameter of recording audio file: sampling rate/Bit solution(8/16bits)/no. of channel
\item Time domain vs Frequency domain
\begin{center}
\begin{tabular}{ccc} 
 & Time & Frequency \\
\hline 
features& Period,Intensity,Timbre(Waveform)& Energy,Pitch,Timbre \\ 

\end{tabular} 
\end{center}
\end{itemize}
\section{Optimization}\label{sec:GD} %DSS Gradient Descent
\begin{itemize}
\item Gradient Descent: $x_{next} = x_{now} -\eta\nabla f(x_{now}) \qquad \eta$: learning rate\\
* not guarantee for global optimum
\item DSS
\end{itemize}
\section{Dynamic Programming} %done
An effective method for finding optimum solution to a multi-stage decision problem.
\begin{enumerate}
\item Characteristic: Decomposition/Subproblem optimality
\item Steps: Optimum-value function $\rightarrow$ Recurrent formula $\rightarrow$ Answer
\item Applications: LCS,Edit distance, matrix chain products,hidden Markov model...
\item Example :QBSH\\
Find the alignment between the pitch vector of human's singing and the note vector of a given melody, both in the unit of semitone or MIDI number. \\
$$ dist(p, q)=\min_{u_1, u_2, ... u_{m}}\sum_{i=1}^{m} |p(i)-q(u_i)|, $$ subject to $u_1=1$ and $u_1 \leq u_2 \leq u_3 \cdots \leq u_{m}$.
\end{enumerate}
\end{document}
